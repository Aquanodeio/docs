---
title: Deploying through Docker
description: Deploy applications using Docker containers on Aquanode
---

import { Step, Steps } from "fumadocs-ui/components/steps";

<Steps>
<Step>

## Navigate to Services

1. From the Aquanode Console, go to [Services](https://console.aquanode.io/workloads/services) under Workloads.  
2. You will see deployment options for:  
   - **GitHub Repo** → Deploy from a GitHub repository  
   - **Docker Image** → Deploy directly from a Docker image  

Select **Docker Image**.  

</Step>
<Step>

## Configure Docker Image

On the Docker configuration screen, enter details of your container:  

- **Docker Image** → e.g., `nginx` or `your-registry.com/org/app`  
- **Tag** → Container tag (default: `latest`)  
- **Port Number (Optional)** → Application port (e.g., `3000`)  

This defines which image will be deployed and how it will be exposed.  

</Step>
<Step>

## Build Settings

Specify basic runtime settings:  

- **CPU Units** → Number of CPUs allocated (e.g., `1`)  
- **Duration (hours)** → Minimum runtime duration (Aquanode will keep the service running for this time unless stopped)  

These settings control how much compute power your service receives and for how long it is provisioned.

</Step>
<Step>

## Resource Settings

Define memory and storage requirements:  

- **Memory** → RAM allocated to the container (e.g., `16 GB`)  
- **Storage** → Persistent disk size (e.g., `40 GB`)  

Optionally, configure GPU settings if your container requires GPU acceleration:  

- **GPU Configuration** → Select from available models (H100, B200, A100, RTX series, etc.).  
- **Provider Filters** → Narrow down to specific providers (e.g., Datacrunch, VoltagePark, Akash).  
- **Refresh** → Updates the list of available GPUs in real time.  

If no GPU is selected, the service runs on CPU-only resources.  

</Step>
<Step>

## Select GPU (Optional)

Scroll down to view available GPU options.  
You can choose:  
- GPU Model (e.g., **B200**, **H100**, **A100**)  
- Provider (Datacrunch, Voltage Park, Akash Network)  
- Region (US, EU, etc.)  

Selecting a GPU ensures your Docker container can leverage GPU acceleration for ML inference, training, or high-performance workloads.  

</Step>
<Step>

## Create Deployment

1. Once configurations are complete, click **Create Deployment**.  
2. Your deployment will appear in the Services list with a status of **Active** or **Pending**.  
3. Deployments may take a few minutes to provision.  

You can track status under **Services → Active/Closed tabs**.  

<Callout title="Note">
  Logs become available once the container is running.
</Callout>

</Step>
</Steps>

---

## Example: Deploying Nginx

- **Docker Image:** `nginx`  
- **Tag:** `latest`  
- **CPU Units:** `1`  
- **Duration:** `1 hour`  
- **Port Number:** `3000`  
- **Memory:** `16 GB`  
- **Storage:** `40 GB`  

This will spin up an **Nginx web server** running on Aquanode infrastructure, accessible via the assigned public endpoint.

---

## Best Practices

- Always specify a **stable image tag** (avoid `latest` in production).  
- Use environment variables for sensitive credentials instead of baking them into the image.  
- Ensure your container listens on the configured **port number**.  
- If GPU is required, confirm your Docker image has CUDA/cuDNN support pre-installed.  

---

## Next Steps

- Manage deployments under **Services Dashboard**.  

With Docker deployment, you can run **any custom backend service** on Aquanode with full flexibility.

